{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Baseline "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "base_path = '/hpf/projects/lsung/projects/mimic4ds/Experiments/baseline'\n",
    "\n",
    "tasks = ['mortality','longlos','invasivevent','sepsis']\n",
    "groups = ['2008 - 2010', '2011 - 2013', '2014 - 2016', '2017 - 2019']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Model Check: 20 nn models per year-group"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "All good.\n"
     ]
    }
   ],
   "source": [
    "check_list = []\n",
    "for task in tasks:\n",
    "    for group in groups:\n",
    "        \n",
    "        fpath = os.path.join(\n",
    "            base_path,\n",
    "            \"artifacts\",\n",
    "            f\"analysis_id={task}\",\n",
    "            \"models\"\n",
    "        )\n",
    "        \n",
    "        fnames = os.listdir(fpath)\n",
    "        fnames = [\n",
    "            x for x in fnames if \n",
    "            'backup' not in x and\n",
    "            'deprecated' not in x\n",
    "        ]\n",
    "        \n",
    "        check = len([\n",
    "            x for x in fnames if\n",
    "            \"_\".join([x.split('_')[1]]) == group\n",
    "        ]) == 20\n",
    "        \n",
    "        if not check:\n",
    "            check_list.append([task,group])\n",
    "\n",
    "if not check_list:\n",
    "    print('All good.')\n",
    "else:\n",
    "    print(check_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Model output check (pred_probs):\n",
    "- folder should contain 10 csv files\n",
    "- Each csv file should contain the following columns: \n",
    "    - phase, outputs, labels, pred_probs, row_ids, ids, iter, analysis_id\n",
    "- maximum value for 'iter' column should be 20\n",
    "- check that the following columns are all integers:\n",
    "    - ids, labels, row_ids \n",
    "- check that the following columns are all floats:\n",
    "    - outputs, pred_probs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "All good.\n"
     ]
    }
   ],
   "source": [
    "cols = ['phase','outputs','pred_probs','labels','row_id','ids','iter','analysis_id']\n",
    "check_list = []\n",
    "for task in tasks:\n",
    "    fpath = os.path.join(\n",
    "            base_path,\n",
    "            \"artifacts\",\n",
    "            f\"analysis_id={task}\",\n",
    "            \"results/pred_probs\"\n",
    "        )\n",
    "    \n",
    "    fnames = [\n",
    "        x for x in os.listdir(fpath) if\n",
    "        'backup' not in x and\n",
    "        'deprecated' not in x\n",
    "    ]\n",
    "    \n",
    "    if len(fnames) != 10:\n",
    "        check_list.append(f\"Missing files in {task}\")\n",
    "        \n",
    "    for fname in fnames:\n",
    "        \n",
    "        df = pd.read_csv(f\"{fpath}/{fname}\")\n",
    "        \n",
    "        if len([\n",
    "            x for x in df.columns if x in cols\n",
    "        ]) != len(cols):\n",
    "            check_list.append(f'Missing columns in {fname} under {task}')\n",
    "            \n",
    "        \n",
    "        if df['iter'].max() != 20:\n",
    "            check_list.append(f'Incorrect max iter in {fname} under {task}')\n",
    "            \n",
    "        # check int columns:\n",
    "        for c in ['ids','labels','row_id']:\n",
    "            if not all(isinstance(x,int) for x in df[c]):\n",
    "                check_list.append(f\"Check column {c} in {fname} under {task}\")\n",
    "        \n",
    "        # check float columns:\n",
    "        for c in ['pred_probs', 'outputs']:\n",
    "            if not all(isinstance(x,float) for x in df[c]):\n",
    "                check_list.append(f\"Check column {c} in {fname} under {task}\")\n",
    "        \n",
    "if not check_list:\n",
    "    print(\"All good.\")\n",
    "else:\n",
    "    for i in check_list:\n",
    "        print(i)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Output evaluation check (evaluate_models):\n",
    "- folder should contain 30 csv files (3 evaluation method for each pred_probs file)\n",
    "- Each csv file should contain the following columns: \n",
    "    - phase, metric, boot_iter, analysis_id\n",
    "    - mean/performance should be float and contains no NaN\n",
    "- maximum value for 'boot_iter' column should be 10000\n",
    "- there should be 5 metrics (ace_abs_logistic_log, ace_rmse_logistic_log, auc, auprc, loss_bce)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "All good.\n"
     ]
    }
   ],
   "source": [
    "cols = ['phase','metric','boot_iter','analysis_id']\n",
    "check_list = []\n",
    "for task in tasks:\n",
    "    fpath = os.path.join(\n",
    "            base_path,\n",
    "            \"artifacts\",\n",
    "            f\"analysis_id={task}\",\n",
    "            \"results/evaluate_models\"\n",
    "        )\n",
    "    \n",
    "    fnames = [\n",
    "        x for x in os.listdir(fpath) if\n",
    "        'backup' not in x and\n",
    "        'deprecated' not in x\n",
    "    ]\n",
    "    \n",
    "    if len(fnames) != 30:\n",
    "        check_list.append(f\"Missing files in {task}\")\n",
    "        \n",
    "    for fname in fnames:\n",
    "        \n",
    "        df = pd.read_csv(f\"{fpath}/{fname}\")\n",
    "        \n",
    "        if len([\n",
    "            x for x in df.columns if x in cols\n",
    "        ]) != len(cols):\n",
    "            check_list.append(f'Missing columns in {fname} under {task}')\n",
    "            \n",
    "        \n",
    "        if df['boot_iter'].max() != 10000:\n",
    "            check_list.append(f'Incorrect max boot_iter in {fname} under {task}')\n",
    "        \n",
    "        if len(df['metric'].unique()) != 5:\n",
    "            check_list.append(f\"Incorrect number of metrics in {fname} under {task}\")\n",
    "            \n",
    "        # check performance columns:\n",
    "        c = [x for x in df.columns if x=='mean' or x=='performance']\n",
    "        if not all(isinstance(x[0],float) for x in df[c].values):\n",
    "            check_list.append(f\"Check column {c} in {fname} under {task}\")\n",
    "        if np.sum(df[c].isnull())[0]>0:\n",
    "            check_list.append(\n",
    "                f\"columns {c} in {fname} under {task} has {np.sum(df[c].isnull())[0]} nan values\")\n",
    "        \n",
    "if not check_list:\n",
    "    print(\"All good.\")\n",
    "else:\n",
    "    for i in check_list:\n",
    "        print(i)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Output comparison check (compare_models):\n",
    "- folder should contain 9 csv files (3 evaluation method for each comparison of baseline (2008 - 2010) and oracle (2017 - 2019))\n",
    "- Each csv file should contain the following columns: \n",
    "    - phase, metric, performance_base, performance_test, performance_diff, boot_iter, analysis_id\n",
    "    - performance_diff should be float and contains no NaN\n",
    "- maximum value for 'boot_iter' column should be 10000\n",
    "- there should be 5 metrics (ace_abs_logistic_log, ace_rmse_logistic_log, auc, auprc, loss_bce)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "All good.\n"
     ]
    }
   ],
   "source": [
    "cols = ['phase','metric','performance_base','performance_test','performance_diff','boot_iter','analysis_id']\n",
    "check_list = []\n",
    "for task in tasks:\n",
    "    fpath = os.path.join(\n",
    "            base_path,\n",
    "            \"artifacts\",\n",
    "            f\"analysis_id={task}\",\n",
    "            \"results/compare_models\"\n",
    "        )\n",
    "    \n",
    "    fnames = [\n",
    "        x for x in os.listdir(fpath) if\n",
    "        'backup' not in x and\n",
    "        'deprecated' not in x\n",
    "    ]\n",
    "    \n",
    "    if len(fnames) != 9:\n",
    "        check_list.append(f\"Missing files in {task}\")\n",
    "        \n",
    "    for fname in fnames:\n",
    "        \n",
    "        df = pd.read_csv(f\"{fpath}/{fname}\")\n",
    "        \n",
    "        if len([\n",
    "            x for x in df.columns if x in cols\n",
    "        ]) != len(cols):\n",
    "            check_list.append(f'Missing columns in {fname} under {task}')\n",
    "            \n",
    "        \n",
    "        if df['boot_iter'].max() != 10000:\n",
    "            check_list.append(f'Incorrect max boot_iter in {fname} under {task}')\n",
    "        \n",
    "        if len(df['metric'].unique()) != 5:\n",
    "            check_list.append(f\"Incorrect number of metrics in {fname} under {task}\")\n",
    "            \n",
    "        # check performance columns:\n",
    "        c = 'performance_diff'\n",
    "        if not all(isinstance(x,float) for x in df[c].values):\n",
    "            check_list.append(f\"Check column {c} in {fname} under {task}\")\n",
    "        if np.sum(df[c].isnull())>0:\n",
    "            check_list.append(\n",
    "                f\"columns {c} in {fname} under {task} has {np.sum(df['performance_diff'].isnull())[0]} nan values\")\n",
    "        \n",
    "if not check_list:\n",
    "    print(\"All good.\")\n",
    "else:\n",
    "    for i in check_list:\n",
    "        print(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
